---
layout: "../../layouts/PostLayout.astro"
pageTitle: "aaronik | deep eval"
title: "Intro to Deep Eval"
slug: "04-deep-eval-intro"
imgSrc: "images/event-sourcing.png"
description: "DeepEval core concepts"
date: "May 3 2025"
authors: ["Aaron Sullivan"]
draft: true
---

## DeepEval: Core Concepts Explained

DeepEval is an open-source framework designed for evaluating and testing large language model (LLM) applications.

**Key Concepts**

- **LLM-Powered Evaluation:** DeepEval uses LLMs themselves (as well as statistical and NLP models) to *evaluate* the responses of other LLMs. This is possible because judging a response is very different from generating one.
- **Test Cases:** At its core, DeepEval treats each evaluation as a test case, similar to unit testing in software. A test case typically includes:
  - *Input*: The user prompt or query.
  - *Actual Output*: The response generated by your LLM application.
  - *Expected Output*: The ideal response.
  - *Context*: Supporting information supplied to your application's LLM, used for context to judge its output.
- **Datasets:** A collection of test cases. This enables bulk evaluation and benchmarking of LLM applications at scale.
- **Metrics:** Methods that measure between 0 and 1 the quality, accuracy, and reliability of AI-generated outputs, including:
  - *G-Eval*: General evaluation metric for correctness.
  - *RAG Metrics*: Answer relevancy, faithfulness, contextual recall/precision, RAGAS.
  - *Agentic Metrics*: Task completion, tool correctness.
  - *Other Metrics*: Hallucination detection, summarization quality, bias, toxicity, knowledge retention, conversation completeness, and more.
  - *Custom Metrics*: Users can define their own metrics for specialized needs.

**Workflow Overview**

1. **Write Test Cases:** Define scenarios with inputs and expected outputs.
2. **Choose Metrics:** Select from built-in or custom metrics tailored to your application type (RAG, chatbot, agent, etc.).
3. **Run Evaluations:** Use DeepEval to score your LLM’s outputs, either individually or in bulk via datasets.
4. **Analyze Results:** Review metric scores (typically 0–1) to determine pass/fail and identify areas for improvement[2].

**Why Use DeepEval?**

- *Objective, repeatable LLM evaluation using LLMs themselves*
- *Supports both local and cloud-based workflows*
- *Seamless integration with CI/CD pipelines*
- *Rich metric library and dataset management for robust testing and benchmarking*

DeepEval makes it easy to systematically judge LLM applications-helping teams move beyond subjective spot-checking to rigorous, automated evaluation[2].

Citations:
[1] https://github.com/confident-ai/deepeval.
[2] https://github.com/confident-ai/deepeval

---
Answer from Perplexity: pplx.ai/share

